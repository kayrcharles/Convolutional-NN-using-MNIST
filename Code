# import the important libraries
import numpy as np # math
import matplotlib.pyplot as plt # graph
import torch # PyTorch
import torch.nn as nn # layers
import torch.optim as optim # optimizer
import torchvision
import torch.nn.functional as F
import torchvision.transforms as transforms # data
from torchvision import datasets, transforms # MNIST is a library from the datasets of torch

# gather data and upload using PyTorch libray already in place
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])

# import test important data
test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000, shuffle=False)

# train data
train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=132, shuffle=True)

# model building, *have seen sequential data examples, is this just a more complex way to do it? or is that the correct way?*
class CNNModel(nn.Module):
  def __init__(self):
    super(CNNModel, self).__init__()
    self.conv1 = nn.Conv2d(1,6,5, padding=2) # inputting the images to a 1 channel images, kernel is equal to 5
    self.pool = nn.MaxPool2d(2,2) # maxpooling is equal to 2, LeNet architecure used padding=2 on the github
    self.conv2 = nn.Conv2d(6,16,5,padding=2) # inputting the images to a 6 channel images, kernel is equal to 5
    self.fc1 = nn.Linear(16*7*7, 120) # first fully-connected layer, 7*7 from image dimension
    self.fc2 = nn.Linear(120, 84) # second fc
    self.fc3 = nn.Linear(84, 10) # output layer, 10 classes

  def forward(self, x):
    x = self.pool(F.relu(self.conv1(x))) # first convolution layer applying the pooling layer as well
    x = self.pool(F.relu(self.conv2(x))) # second convolution layer applying the pooling layer as well
    x = torch.flatten(x, 1) # flatten the tensor before you can apply the linear function
    x = F.relu(self.fc1(x)) # first fully-connected layer, no pooling
    x = F.relu(self.fc2(x)) # second fully-connected layer, no pooling
    x = self.fc3(x) # output layer, no relu
    return x

cnn = CNNModel()

# loss and learning rate
learningRate = 0.001
loss_function = nn.CrossEntropyLoss() # Rename the loss function variable
optimizer = optim.SGD(cnn.parameters(), lr=learningRate, momentum = 0.9) # optimizer

# training
num_epochs = 5
for epoch in range(num_epochs):
  cnn.train()
  runningLoss = 0.0
  for i, (inputs, labels) in enumerate(train_loader):
    optimizer.zero_grad()
    outputs = cnn(inputs)
    loss = loss_function(outputs, labels) # Use the original loss function variable here
    loss.backward()
    optimizer.step()

    runningLoss += loss.item()

  print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')

# evaluation
cnn.eval()
incorrect_cases = []
total = 0
correct = 0

with torch.no_grad():
    for inputs, labels in test_loader:
        outputs = cnn(inputs)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

        for i in range(len(labels)):
            if predicted[i] != labels[i]:
                incorrect_cases.append((inputs[i], labels[i], outputs[i]))

print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')
